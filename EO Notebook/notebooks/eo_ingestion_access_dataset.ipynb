{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893dd4be",
   "metadata": {},
   "source": [
    "# EO Notebook: Ingestion, Access Patterns, Dataset Engineering\n",
    "\n",
    "Questo notebook mostra una pipeline EO completa, orientata a dimostrare competenze pratiche di **data engineering geospaziale** e preparazione dati per AI.\n",
    "Il flusso segue una logica di produzione: discovery -> accesso efficiente -> preprocessing -> packaging -> quality checks -> data loading.\n",
    "\n",
    "## Obiettivi tecnici\n",
    "\n",
    "1. **Data discovery** via STAC, con filtri riproducibili su AOI, tempo e cloud cover.\n",
    "2. **Data access** da COG con pattern efficienti (windowed read e range request), evitando letture full-raster non necessarie.\n",
    "3. **Dataset engineering** per creare artefatti AI-ready scalabili e riusabili.\n",
    "\n",
    "## Perche questo approccio\n",
    "\n",
    "- Separa la logica configurabile (`config.yaml`) dal codice.\n",
    "- Produce output standard nel dominio EO/ML (`Zarr`, `GeoParquet`, `JSON metadata`).\n",
    "- Rende il workflow ripetibile e verificabile tramite report QC.\n",
    "\n",
    "## Deliverable esportati\n",
    "\n",
    "- `config.yaml`\n",
    "- `src/ingest_stac.py`\n",
    "- `data/tiles.parquet` (GeoParquet tile index)\n",
    "- `artifacts/statistics.json` (mean/std per banda)\n",
    "- `data/data.zarr` (packaging scalabile)\n",
    "- `artifacts/metadata.json`\n",
    "- `reports/qc_report.md` + `reports/qc_report.html`\n",
    "- `src/dataloader.py` + snippet d'uso PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404d50c3-01aa-4c90-9ebf-3bb78e69aa97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "\n",
    "# Change to the 'notebook' directory in Google Drive\n",
    "notebook_path = '/content/drive/My Drive/Notebooks/EO'\n",
    "if not os.path.exists(notebook_path):\n",
    "    os.makedirs(notebook_path) # Create the directory if it doesn't exist\n",
    "os.chdir(notebook_path)\n",
    "print(f\"Current working directory changed to: {os.getcwd()}\")\n",
    "\n",
    "!pip install rioxarray pystac-client planetary-computer stackstac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770eeb50",
   "metadata": {},
   "source": [
    "## 0) Setup ambiente\n",
    "\n",
    "In questa sezione inizializziamo l'ambiente, carichiamo la configurazione e prepariamo le cartelle output.\n",
    "L'obiettivo e rendere la pipeline **parametrica** e facilmente rieseguibile su AOI o periodi diversi.\n",
    "\n",
    "### Librerie chiave\n",
    "\n",
    "- `pystac-client`: interrogazione cataloghi STAC.\n",
    "- `rasterio` / `rioxarray` / `xarray`: I/O raster, georeferenziazione, manipolazione array.\n",
    "- `stackstac`: costruzione stack spazio-temporali consistenti a partire da STAC Items.\n",
    "- `geopandas` / `shapely`: gestione geometrie AOI e tile index.\n",
    "\n",
    "### Output atteso\n",
    "\n",
    "- Config caricata da `config.yaml`.\n",
    "- Directory `data/`, `artifacts/`, `reports/` create (se mancanti).\n",
    "- Config risalvata per tracciare con precisione i parametri di run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad80a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import sys # Import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import yaml\n",
    "from shapely.geometry import box\n",
    "sys.path.insert(0, str(Path().resolve()))\n",
    "\n",
    "from src.ingest_stac import (\n",
    "    get_asset_href,\n",
    "    items_to_frame,\n",
    "    load_config,\n",
    "    read_cog_window,\n",
    "    save_config,\n",
    "    search_stac_items,\n",
    "    to_geodataframe,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "cfg = load_config(CONFIG_PATH)\n",
    "\n",
    "DATA_DIR = Path(cfg[\"io\"][\"data_dir\"])\n",
    "ARTIFACT_DIR = Path(cfg[\"io\"][\"artifact_dir\"])\n",
    "REPORT_DIR = Path(cfg[\"io\"][\"report_dir\"])\n",
    "\n",
    "for d in [DATA_DIR, ARTIFACT_DIR, REPORT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Re-export della configurazione come deliverable riproducibile\n",
    "save_config(cfg, CONFIG_PATH)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099ff21",
   "metadata": {},
   "source": [
    "## 1) Data Discovery (STAC)\n",
    "\n",
    "Qui interroghiamo il catalogo STAC con tre vincoli principali:\n",
    "\n",
    "- **AOI**: area geografica di interesse (GeoJSON in `config.yaml`).\n",
    "- **Intervallo temporale**: finestra di osservazione.\n",
    "- **Cloud cover**: filtro qualità iniziale per Sentinel-2.\n",
    "\n",
    "### Cosa validiamo subito\n",
    "\n",
    "- Numero di scene trovate.\n",
    "- Distribuzione temporale (`datetime`) per verificare copertura del periodo.\n",
    "- CRS/EPSG disponibili negli item, utile per decidere il sistema target dello stack.\n",
    "\n",
    "Questa fase e critica: se i filtri sono troppo stretti, il dataset diventa povero; se troppo larghi, aumentano costi di I/O e rumore nei dati.\n",
    "\n",
    "### Nota accesso Planetary Computer\n",
    "\n",
    "Gli asset su Azure Blob richiedono URL firmate (SAS).\n",
    "In questo notebook la firma viene applicata automaticamente agli item STAC; senza firma puoi ricevere errori HTTP `409` / `403` in `rasterio` o `stackstac`.\n",
    "\n",
    "Per uso base di norma **non serve un token manuale**. Una subscription key e opzionale per workload intensivi (rate limit più stabili)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65618524",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = search_stac_items(cfg)\n",
    "\n",
    "# Sign URLs for Planetary Computer assets (SAS) to avoid HTTP 409/403 on blob access\n",
    "if cfg[\"stac\"][\"provider\"].lower() in {\"planetary-computer\", \"pc\", \"microsoft\"}:\n",
    "    try:\n",
    "        import planetary_computer as pc\n",
    "        items = [pc.sign(item) for item in items]\n",
    "    except Exception as e:\n",
    "        print(\"Warning: non sono riuscito a firmare gli item:\", e)\n",
    "\n",
    "items_df = items_to_frame(items)\n",
    "\n",
    "print(f\"Items trovati: {len(items)}\")\n",
    "if len(items_df):\n",
    "    display(items_df.head(10))\n",
    "    print(\"Range temporale:\", items_df[\"datetime\"].min(), \"->\", items_df[\"datetime\"].max())\n",
    "    print(\"Cloud cover medio:\", round(items_df[\"cloud_cover\"].dropna().mean(), 2))\n",
    "else:\n",
    "    raise RuntimeError(\"Nessun item trovato: allarga date/AOI o aumenta cloud cover max.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf4f51",
   "metadata": {},
   "source": [
    "### Opzionale: estensione a Sentinel-1\n",
    "\n",
    "Per una pipeline multi-sensore puoi aggiungere `sentinel-1-rtc` alle collection.\n",
    "In quel caso conviene definire chiaramente una strategia di fusione (ottico + radar):\n",
    "\n",
    "- allineamento spaziale/temporale tra sensori,\n",
    "- normalizzazione per domini radiometrici diversi,\n",
    "- definizione feature finali per il modello.\n",
    "\n",
    "In questa demo manteniamo solo Sentinel-2 L2A per focalizzarci su cloud masking con banda `SCL` e tenere il percorso didattico lineare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448b27d",
   "metadata": {},
   "source": [
    "## 2) Data Access Pattern (COG + HTTP range)\n",
    "\n",
    "L'obiettivo non e leggere l'intera immagine, ma mostrare accesso efficiente a blocchi utili:\n",
    "\n",
    "- **Windowed read**: estraiamo solo la porzione che interseca l'AOI.\n",
    "- **Cloud-native access**: i COG supportano letture parziali via HTTP range request.\n",
    "\n",
    "### Perche e importante\n",
    "\n",
    "Su dataset EO grandi, il collo di bottiglia e spesso I/O remoto.\n",
    "Leggere solo i byte necessari riduce latenza, banda e costo operativo.\n",
    "Questo pattern e la base per pipeline scalabili in cloud e training distribuito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = items[0]\n",
    "provider = cfg[\"stac\"][\"provider\"]\n",
    "example_band = \"B04\"\n",
    "asset_href = get_asset_href(first_item, example_band, provider)\n",
    "\n",
    "aoi_gdf = to_geodataframe(cfg)\n",
    "aoi_bounds = tuple(aoi_gdf.total_bounds)\n",
    "\n",
    "window_array, window_profile = read_cog_window(\n",
    "    asset_href=asset_href,\n",
    "    bounds=aoi_bounds,\n",
    "    bounds_crs=\"EPSG:4326\",\n",
    "    band=1,\n",
    ")\n",
    "\n",
    "print(\"Asset:\", asset_href[:120] + \"...\")\n",
    "print(\"Shape window letta:\", window_array.shape)\n",
    "print(\"Profilo sintetico:\", window_profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8f95d",
   "metadata": {},
   "source": [
    "### Verifica pratica della range request (opzionale)\n",
    "\n",
    "Questa cella invia una richiesta HTTP con header `Range` per scaricare solo un piccolo intervallo di byte.\n",
    "Su endpoint COG compatibili, la risposta attesa e in genere `206 Partial Content`.\n",
    "\n",
    "Nota: alcuni gateway/CDN possono comportarsi diversamente pur mantenendo la lettura efficiente lato GDAL/rasterio.\n",
    "Per questo la verifica e utile come check didattico, ma non sostituisce il profiling end-to-end del workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    r = requests.get(asset_href, headers={\"Range\": \"bytes=0-1023\"}, timeout=30)\n",
    "    print(\"HTTP status:\", r.status_code)\n",
    "    print(\"Content-Range:\", r.headers.get(\"Content-Range\"))\n",
    "    print(\"Bytes ricevuti:\", len(r.content))\n",
    "except Exception as e:\n",
    "    print(\"Range request demo non eseguita:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b8998",
   "metadata": {},
   "source": [
    "## 3) Preprocessing: align, cloud mask, normalization\n",
    "\n",
    "In questa fase trasformiamo scene STAC eterogenee in un cubo coerente e utilizzabile per ML.\n",
    "\n",
    "### Step\n",
    "\n",
    "1. Costruiamo uno stack spazio-temporale allineato con `stackstac` (stessa griglia/resolution/CRS).\n",
    "2. Selezioniamo bande ottiche (`B02/B03/B04/B08`) e la banda `SCL` per quality mask.\n",
    "3. Applichiamo masking di cloud/shadow/classi non valide (configurabili in `config.yaml`).\n",
    "4. Normalizziamo i DN in riflettanza (`/10000`).\n",
    "5. Creiamo un composito temporale robusto (mediana) per ridurre rumore residuo.\n",
    "\n",
    "### Nota tecnica\n",
    "\n",
    "`fill_value` viene impostato come `np.float32(np.nan)` per essere compatibile con `dtype='float32'` in `stackstac`.\n",
    "Questo evita errori di cast e mantiene footprint memoria contenuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stackstac\n",
    "\n",
    "assets = cfg[\"stac\"][\"assets\"]\n",
    "optical_bands = [b for b in assets if b != \"SCL\"]\n",
    "invalid_scl = np.array(cfg[\"preprocessing\"][\"cloud_mask\"][\"scl_invalid_classes\"])\n",
    "\n",
    "if items_df[\"epsg\"].dropna().empty:\n",
    "    target_epsg = 4326\n",
    "else:\n",
    "    target_epsg = int(items_df[\"epsg\"].dropna().mode().iloc[0])\n",
    "\n",
    "# Refresh signed URLs before stacking (useful if notebook stays open for long)\n",
    "if cfg[\"stac\"][\"provider\"].lower() in {\"planetary-computer\", \"pc\", \"microsoft\"}:\n",
    "    try:\n",
    "        import planetary_computer as pc\n",
    "        items = [pc.sign(item) for item in items]\n",
    "    except Exception as e:\n",
    "        print(\"Warning: refresh signing skipped:\", e)\n",
    "\n",
    "stack = stackstac.stack(\n",
    "    items,\n",
    "    assets=assets,\n",
    "    epsg=target_epsg,\n",
    "    resolution=cfg[\"preprocessing\"][\"target_resolution\"],\n",
    "    bounds_latlon=tuple(aoi_gdf.total_bounds),\n",
    "    chunksize=1024,\n",
    "    dtype=\"float32\",\n",
    "    fill_value=np.float32(np.nan),\n",
    "    rescale=False,\n",
    ")\n",
    "\n",
    "reflectance = stack.sel(band=optical_bands)\n",
    "scl = stack.sel(band=\"SCL\")\n",
    "\n",
    "valid_mask = ~xr.apply_ufunc(\n",
    "    np.isin,\n",
    "    scl,\n",
    "    invalid_scl,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[bool],\n",
    ")\n",
    "\n",
    "reflectance = reflectance.where(valid_mask)\n",
    "scale = float(cfg[\"preprocessing\"][\"normalization\"][\"reflectance_scale\"])\n",
    "reflectance = reflectance / scale\n",
    "\n",
    "# Composito temporale robusto\n",
    "composite = reflectance.median(dim=\"time\", skipna=True).expand_dims(time=[np.datetime64(cfg[\"stac\"][\"date_range\"][\"end\"])])\n",
    "composite = composite.astype(\"float32\")\n",
    "\n",
    "print(composite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac4fde",
   "metadata": {},
   "source": [
    "## 4) Tiling + split train/val\n",
    "\n",
    "Convertiamo il composito in tile regolari (es. 256x256) e costruiamo un indice geospaziale.\n",
    "\n",
    "### Contenuto del tile index\n",
    "\n",
    "- coordinate pixel (`x0`, `y0`) per slicing nel cubo,\n",
    "- `tile_id` univoco,\n",
    "- `geometry` reale del tile (utile per join con label o analisi GIS),\n",
    "- split `train` / `val` deterministico.\n",
    "\n",
    "Questa separazione tra **array dati** e **indice tile** semplifica training lazy, tracciabilita e integrazione con labeling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = int(cfg[\"preprocessing\"][\"tile_size\"])\n",
    "train_fraction = float(cfg[\"preprocessing\"][\"train_fraction\"])\n",
    "\n",
    "height = composite.sizes[\"y\"]\n",
    "width = composite.sizes[\"x\"]\n",
    "\n",
    "x_coords = composite[\"x\"].values\n",
    "y_coords = composite[\"y\"].values\n",
    "\n",
    "dx = float(np.abs(np.diff(x_coords[:2])).mean()) if len(x_coords) > 1 else float(cfg[\"preprocessing\"][\"target_resolution\"])\n",
    "dy = float(np.abs(np.diff(y_coords[:2])).mean()) if len(y_coords) > 1 else float(cfg[\"preprocessing\"][\"target_resolution\"])\n",
    "\n",
    "records = []\n",
    "\n",
    "for y0 in range(0, height - tile_size + 1, tile_size):\n",
    "    for x0 in range(0, width - tile_size + 1, tile_size):\n",
    "        tile_id = f\"t0_y{y0}_x{x0}\"\n",
    "        split = \"train\" if (abs(hash(tile_id)) % 100) < int(train_fraction * 100) else \"val\"\n",
    "\n",
    "        x_min = float(x_coords[x0] - dx / 2)\n",
    "        x_max = float(x_coords[x0 + tile_size - 1] + dx / 2)\n",
    "        y_top = float(y_coords[y0] + dy / 2)\n",
    "        y_bottom = float(y_coords[y0 + tile_size - 1] - dy / 2)\n",
    "\n",
    "        geom = box(min(x_min, x_max), min(y_bottom, y_top), max(x_min, x_max), max(y_bottom, y_top))\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"tile_id\": tile_id,\n",
    "                \"time_idx\": 0,\n",
    "                \"x0\": x0,\n",
    "                \"y0\": y0,\n",
    "                \"split\": split,\n",
    "                \"geometry\": geom,\n",
    "            }\n",
    "        )\n",
    "\n",
    "tiles_gdf = gpd.GeoDataFrame(records, geometry=\"geometry\", crs=f\"EPSG:{target_epsg}\")\n",
    "print(\"Numero tile:\", len(tiles_gdf))\n",
    "display(tiles_gdf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2cc72",
   "metadata": {},
   "source": [
    "## 5) Packaging AI-ready: Zarr + GeoParquet + metadata\n",
    "\n",
    "Qui serializziamo il dataset in formati adatti a workload ML su larga scala.\n",
    "\n",
    "### Artefatti prodotti\n",
    "\n",
    "- `data.zarr`: cubo multidimensionale chunked, ottimo per accesso lazy/parallelo.\n",
    "- `tiles.parquet`: indice tile in formato tabellare geospaziale.\n",
    "- `statistics.json`: statistiche per banda (mean/std) per normalizzazione consistente.\n",
    "- `metadata.json`: contesto del run (AOI, date, collection, CRS, path output, cardinalita).\n",
    "\n",
    "Il vantaggio e avere un dataset riproducibile e pronto sia per training locale sia per pipeline distribuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca315655",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path = Path(cfg[\"io\"][\"zarr_path\"])\n",
    "tiles_path = Path(cfg[\"io\"][\"tiles_index_path\"])\n",
    "stats_path = Path(cfg[\"io\"][\"stats_path\"])\n",
    "metadata_path = Path(cfg[\"io\"][\"metadata_path\"])\n",
    "\n",
    "zarr_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "tiles_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "stats_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "metadata_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset Xarray per export Zarr\n",
    "cube = composite.chunk({\"time\": 1, \"band\": len(optical_bands), \"y\": tile_size, \"x\": tile_size})\n",
    "ds = xr.Dataset({\"cube\": cube})\n",
    "ds.to_zarr(zarr_path, mode=\"w\")\n",
    "\n",
    "# GeoParquet tile index\n",
    "tiles_gdf.to_parquet(tiles_path, index=False)\n",
    "\n",
    "# Statistiche per banda\n",
    "mean = cube.mean(dim=(\"time\", \"y\", \"x\"), skipna=True).compute().values.tolist()\n",
    "std = cube.std(dim=(\"time\", \"y\", \"x\"), skipna=True).compute().values.tolist()\n",
    "\n",
    "stats_payload = {\n",
    "    \"bands\": optical_bands,\n",
    "    \"mean\": [float(x) for x in mean],\n",
    "    \"std\": [float(x) if float(x) > 1e-6 else 1.0 for x in std],\n",
    "    \"tile_size\": tile_size,\n",
    "}\n",
    "with stats_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats_payload, f, indent=2)\n",
    "\n",
    "metadata = {\n",
    "    \"aoi\": cfg[\"aoi\"][\"name\"],\n",
    "    \"collections\": cfg[\"stac\"][\"collections\"],\n",
    "    \"date_range\": cfg[\"stac\"][\"date_range\"],\n",
    "    \"provider\": cfg[\"stac\"][\"provider\"],\n",
    "    \"num_items\": int(len(items)),\n",
    "    \"num_tiles\": int(len(tiles_gdf)),\n",
    "    \"bands\": optical_bands,\n",
    "    \"zarr_path\": str(zarr_path),\n",
    "    \"tiles_index_path\": str(tiles_path),\n",
    "    \"stats_path\": str(stats_path),\n",
    "    \"crs\": f\"EPSG:{target_epsg}\",\n",
    "}\n",
    "with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Export completato:\")\n",
    "print(\"-\", zarr_path)\n",
    "print(\"-\", tiles_path)\n",
    "print(\"-\", stats_path)\n",
    "print(\"-\", metadata_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a337d8",
   "metadata": {},
   "source": [
    "## 6) Quality checks e report\n",
    "\n",
    "Eseguiamo controlli base ma operativi per stimare qualita e affidabilita del dataset:\n",
    "\n",
    "- **Nodata ratio**: quantifica dati mancanti post-masking.\n",
    "- **Band completeness**: copertura informativa per ciascuna banda.\n",
    "- **CRS consistency**: verifica coerenza dei sistemi di riferimento tra item.\n",
    "- **Outlier ratio**: stima valori anomali dopo standardizzazione.\n",
    "- **Tile coverage**: rapporto tra tile generati e tile attesi sulla griglia.\n",
    "\n",
    "Le metriche vengono esportate in report Markdown/HTML per audit tecnico, handoff team e confronto tra run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_md_path = Path(cfg[\"io\"][\"qc_report_md\"])\n",
    "qc_html_path = Path(cfg[\"io\"][\"qc_report_html\"])\n",
    "qc_md_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "qc_html_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Metriche principali\n",
    "missing_ratio = float(cube.isnull().mean().compute())\n",
    "band_completeness = (1.0 - cube.isnull().mean(dim=(\"time\", \"y\", \"x\")).compute()).values\n",
    "\n",
    "crs_unique = int(items_df[\"epsg\"].dropna().astype(str).nunique())\n",
    "crs_status = \"OK\" if crs_unique <= 1 else \"CHECK\"\n",
    "\n",
    "mean_arr = np.array(stats_payload[\"mean\"])\n",
    "std_arr = np.array(stats_payload[\"std\"])\n",
    "z = (cube - xr.DataArray(mean_arr, dims=[\"band\"])) / xr.DataArray(std_arr, dims=[\"band\"])\n",
    "outlier_ratio = float((np.abs(z) > 6).mean().compute())\n",
    "\n",
    "expected_tiles = (height // tile_size) * (width // tile_size)\n",
    "coverage_ratio = float(len(tiles_gdf) / expected_tiles) if expected_tiles else 0.0\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\"check\": \"nodata_ratio\", \"value\": missing_ratio, \"status\": \"OK\" if missing_ratio < 0.4 else \"CHECK\"},\n",
    "        {\"check\": \"crs_consistency\", \"value\": crs_unique, \"status\": crs_status},\n",
    "        {\"check\": \"outlier_ratio_abs_z_gt_6\", \"value\": outlier_ratio, \"status\": \"OK\" if outlier_ratio < 0.01 else \"CHECK\"},\n",
    "        {\"check\": \"tile_coverage_ratio\", \"value\": coverage_ratio, \"status\": \"OK\" if coverage_ratio > 0.95 else \"CHECK\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "bands_df = pd.DataFrame(\n",
    "    {\n",
    "        \"band\": optical_bands,\n",
    "        \"completeness\": band_completeness,\n",
    "        \"mean\": stats_payload[\"mean\"],\n",
    "        \"std\": stats_payload[\"std\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "report_md = \"\"\"# QC Report - EO Dataset\\n\\n## Summary\\n\"\"\" + summary_df.to_markdown(index=False) + \"\"\"\\n\\n## Band Statistics\\n\"\"\" + bands_df.to_markdown(index=False)\n",
    "\n",
    "qc_md_path.write_text(report_md, encoding=\"utf-8\")\n",
    "\n",
    "report_html = (\n",
    "    \"<html><head><meta charset='utf-8'><title>EO QC Report</title></head><body>\"\n",
    "    \"<h1>QC Report - EO Dataset</h1>\"\n",
    "    \"<h2>Summary</h2>\" + summary_df.to_html(index=False) +\n",
    "    \"<h2>Band Statistics</h2>\" + bands_df.to_html(index=False) +\n",
    "    \"</body></html>\"\n",
    ")\n",
    "qc_html_path.write_text(report_html, encoding=\"utf-8\")\n",
    "\n",
    "display(summary_df)\n",
    "display(bands_df)\n",
    "print(\"Report salvato in:\", qc_md_path, \"e\", qc_html_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51808a7d",
   "metadata": {},
   "source": [
    "## 7) Access pattern per training: DataLoader PyTorch\n",
    "\n",
    "Questa sezione dimostra un pattern pratico di integrazione con training loop.\n",
    "\n",
    "### Come funziona\n",
    "\n",
    "- Il dataset legge l'indice tile da `tiles.parquet`.\n",
    "- Ogni `__getitem__` apre/slicea lazy il blocco corrispondente in `data.zarr`.\n",
    "- La normalizzazione usa `statistics.json`, garantendo coerenza tra train/val/inferenza.\n",
    "\n",
    "### Perche scala\n",
    "\n",
    "Non serve materializzare tutto in RAM: i batch vengono costruiti on-demand.\n",
    "Questo approccio e adatto a dataset EO molto grandi, multi-temporali o multi-area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader import EOTileDataset, make_dataloader\n",
    "\n",
    "train_dataset = EOTileDataset(\n",
    "    tile_index_path=tiles_path,\n",
    "    zarr_path=zarr_path,\n",
    "    stats_path=stats_path,\n",
    "    split=\"train\",\n",
    "    tile_size=tile_size,\n",
    "    feature_name=\"cube\",\n",
    ")\n",
    "\n",
    "print(\"Numero sample train:\", len(train_dataset))\n",
    "if len(train_dataset):\n",
    "    sample = train_dataset[0]\n",
    "    print(\"Shape sample image [C,H,W]:\", tuple(sample[\"image\"].shape))\n",
    "\n",
    "train_loader = make_dataloader(\n",
    "    tile_index_path=tiles_path,\n",
    "    zarr_path=zarr_path,\n",
    "    stats_path=stats_path,\n",
    "    split=\"train\",\n",
    "    tile_size=tile_size,\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "    feature_name=\"cube\",\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch image shape [B,C,H,W]:\", tuple(batch[\"image\"].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e599819",
   "metadata": {},
   "source": [
    "## 8) Recap finale\n",
    "\n",
    "Con questo notebook hai una demo completa di EO data engineering, centrata sui pattern richiesti in ruolo:\n",
    "\n",
    "- discovery STAC configurabile,\n",
    "- accesso COG cloud-native,\n",
    "- preprocessing robusto e tracciabile,\n",
    "- packaging AI-ready standard,\n",
    "- quality checks esportabili,\n",
    "- data loader PyTorch lazy.\n",
    "\n",
    "### Evoluzioni naturali verso produzione\n",
    "\n",
    "1. Orchestrazione batch (Prefect/Airflow) con scheduling.\n",
    "2. Versionamento dataset/metadata (es. DVC o catalog interno).\n",
    "3. Integrazione label pipeline e monitoraggio quality drift nel tempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
